---
title: "USED CAR PRICES CASE STUDY"
subtitle: 'Deliverable 3: General and Binary/Logistic Regression Models'
author: "Miquel Parra i Xavier Alaman"
date: \today
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

# R libraries imports, useful functions and data loading

In this first section we will load all required packages and libraries, and load our data.

## Load Required Packages

```{r, message=FALSE, warning=FALSE, results='hide'}
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))

requiredPackages <- c("effects","FactoMineR","car", "factoextra","RColorBrewer","ggplot2","dplyr","ggmap","ggthemes","knitr")

#use this function to check if each package is on the local machine
#if a package is installed, it will be loaded
#if any are not, the missing package(s) will be installed and loaded
package.check <- lapply(requiredPackages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})
#verify they are loaded
search()

```

## Sample load

```{r, results='hide'}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

# Users file path
miquel_fp <- "C:/Users/Miquel/Documents/GitHub/ADEI/"
xavi_fp <- "~/Documents/FIB/ADEI/ADEI/"
filepath <- xavi_fp
filepath <- miquel_fp 

# Set working directory
setwd(filepath)

# Load data from file
load(paste0(filepath,"MyOldCars-5000Clean2.RData"))
# Index reset
row.names(df) <- NULL
```

## Preparing the data 
We cannot have values equal to zero in the variables because in case we apply a logarithmic transformation to them, this would give an error since the logarithm of zero is undefined.
```{r}
names(df)
summary(df$engineSize)
levels(df$engineSize)
vars_con<-names(df)[c(5,7,8,11)]
vars_res<-names(df)[c(3,19)]
vars_dis<-names(df)[c(1,2,4,6,9,10,12,14,15,16,17,18)]
ll<-which(df$years_sell==0);ll
df$years_sell[ll]<-0.5

ll<-which(df$tax==0);ll
df$tax[ll]<-0.5

ll<-which(df$mileage==0);ll
df$mileage[ll]<-0.5

ll<-which(df$mpg==0);ll
df$mpg[ll]<-0.5
```

# Linear Models
We do linear regression in order to predict the value of price variable based on/according to the values of other variables.

## Only Numeric variables
```{r}
m1<-lm(price~mileage+tax+mpg+years_sell,data=df)
summary(m1)
vif(m1)
par(mfrow=c(2,2))
plot(m1,id.n=0)
residualPlots(m1,id=list(method=cooks.distance(m1),n=10)) 
```

As we can see all the chosen variables have coefficients different from zero, i.e. they are useful, because their p-values are less than 0.05.
These explanatory variables explain a 52.4 % of the target's variability. 
Also, we can see that there are no collinear variables, i.e., highly correlated variables.

However, we can see that residuals are neither homoscedastic nor normal. So, we have to apply transformations. Also, the clearest nonlinear variable is mpg. 

### Target transformation
```{r}
library(MASS)
# Target variable transformation?
par(mfrow=c(1,1))
boxcox(price~mileage+tax+mpg+years_sell,data=df)
# Lambda=0 - log transformation is needed

# New model:
m2<-lm(log(price)~mileage+tax+mpg+years_sell,data=df)
summary(m2)
vif(m2) 
par(mfrow=c(2,2))
plot(m2, id.n=0)
residualPlots(m2,id=list(method=cooks.distance(m2),n=10)) 
```

As we can see the transformation needed is a logarithmic transformation, because λ ≈ 0. 

Now, we can see that tax variable is not useful, because its p-value is greater than 0.05, so we should remove it but we won't do it because applying a transformation to it we will make it useful. However, the explanatory variables explain a 61.18 % of the target's variability.
There are still no collinear variables because they have not changed. 

However, the residuals continue to be neither homoscedastic nor normal. So, more/further transformations are needed/required. Also, the clearest nonlinear variables are mpg and years_sell. 

This model is better than the previous one.
It seems that this model is better than the previous one.

### Explanatory variables transformation
```{r}
boxTidwell(log(price)~mileage+tax+mpg, data=df[!df$mout=="YesMOut",])
#?boxTidwell
```

We need to cube the tax variable, take the square root of the mileage variable and take the square root and raise to minus one the mpg variable.

```{r}
#df2 <- df[!df$mout=="YesMOut",]

m3<-lm(log(price)~sqrt(mileage)+poly(tax,3)+ I(mpg^(-1/2))+years_sell,data=df[!df$mout=="YesMOut",])

summary(m3)
vif(m3) 
par(mfrow=c(2,2))
plot(m3, id.n=0)
residualPlots(m3,id=list(method=cooks.distance(m3),n=10)) 
```

As we can see all the chosen variables have coefficients different from zero, i.e. they are useful, because their p-values are less than 0.05.
These explanatory variables explain a 56.51 % of the target's variability. 
We can say that the mileage and years_sell variables are collinear, i.e., they are highly correlated. However, this is because we have adapted them.

Also, we can see that residuals are now homoscedastic and normal. However, we can see some influential observation. Also, there are no excessively non-linear variables

This model is better than the previous one.
It seems that this model is better than the previous one.

## Including factors
```{r}
m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
summary(m4)
vif(m4)
#acf(rstudent(m4))
par(mfrow=c(2,2))
plot(m4)
#residualPlots(m4,id=list(method=cooks.distance(m4),n=10)) 
plot(allEffects(m4), selection = 5)
plot(allEffects(m4), selection = 6)
plot(allEffects(m4), selection = 7)
plot(allEffects(m4), selection = 8)

```

These explanatory variables explain an 83.86 % of the target's variability. 

We can say that the mileage and years_sell variables are still collinear, i.e., they are highly correlated. However, this is because we have adapted them.
Also, we can see that residuals are still homoscedastic and normal. However, we can see some influential observation. 

We can see:
The fact that a car is semi-automatic makes the logarithm of the price increase by 0.17 units. 
The fact that a car is automatic makes the logarithm of the price increase by 0.15 units. 
The effect of being a hybrid car, an other car and a diesel car (baseline) is the same, because their p-values are greater than 0.05.
The fact that a car is petrol makes the logarithm of the price decrease by 0.24 units. 
The fact that a car is BWM makes the logarithm of the price decrease by 0.045 units.
The effect of being a Mercedes car and an Audi car (baseline) is the same, because its p-value is greater than 0.05.
The effect that a car is VW makes the logarithm of the price decrease by 0.21 units.
The effect that a car have medium_engine makes the logarithm of the price decrease by 0.19 units.
The effect that a car have small_engine makes the logarithm of the price decrease by 0.29 units.

## Interactions
### Factors interaction
We are going to see if depending on the fuelType of the car and its aux_tax the price changes/varies. 
```{r}
m5<-lm(log(price)~sqrt(mileage)+fuelType*aux_tax+poly(tax,3)+I(mpg^(-1/2))+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
summary(m5)
plot(allEffects(m5), selection = 8)
```
We can say that the price doesn't changes/varies depending on the fuelType and its aux_tax, because their p-values are greater than 0.05.

```{r}
m5<-lm(log(price)~sqrt(mileage)+fuelType*aux_tax+poly(tax,3)+I(mpg^(-1/2))+years_sell+transmission,data=df[!df$mout=="YesMOut",])
summary(m5)
plot(allEffects(m5), selection = 6)
```
We can say that the price doesn't changes/varies depending on the fuelType and its aux_tax, because their p-values are greater than 0.05.


### Factor and covariate interaction
We are going to see if depending on the fuelType and its mpg the price changes/varies.
```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*mpg+poly(tax,3)+years_sell+transmission,data=df[!df$mout=="YesMOut",])
summary(m6)
plot(allEffects(m6), selection = 5)
```
We can see:
The fact that a car is hybrid and with its mpg makes the logarithm of the price increase by 0.0078 units.
The effect of being an other car with its mpg and a diesel car with its mpg (baseline) is the same, because its p-value is greater than 0.05.
The fact that a car is petrol and with its mpg makes the logarithm of the price decrease by 0.010 units.

```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*mpg+poly(tax,3)+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
summary(m6)
plot(allEffects(m6), selection = 7)
```
We can see:
The fact that a car is hybrid and with its mpg makes the logarithm of the price increase by 0.0053 units.
The effect of being an other car with its mpg and a diesel car with its mpg (baseline) is the same, because its p-value is greater than 0.05.
The fact that a car is petrol and with its mpg makes the logarithm of the price decrease by 0.0066 units.

```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*I(mpg^(-1/2))+poly(tax,3)+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
summary(m6)
plot(allEffects(m6), selection = 7)
```
We can see:
The fact that a car is hybrid and with its mpg makes the logarithm of the price decrease by 9.52 units.
The effect of being a other car with its mpg and a diesel car with its mpg (baseline) is the same, because its p-value is greater than 0.05.
The fact that a car is petrol and with its mpg makes the logarithm of the price increase by 1.36 units.

```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*I(mpg^(-1/2))+poly(tax,3)+years_sell+transmission,data=df[!df$mout=="YesMOut",])
summary(m6)
plot(allEffects(m6), selection = 5)
```
We can see:
The fact that a car is hybrid and with its mpg makes the logarithm of the price decrease by 9.64 units.
The effect of being a other car with its mpg and a diesel car with its mpg (baseline) is the same, because its p-value is greater than 0.05.
The fact that a car is petrol and with its mpg makes the logarithm of the price increase by 3.55 units.

## Diagnostics 
A good model should be consistent with theoretical properties in residual analysis. Neither influential
nor unusual data should be included.
```{r}
dfwork <- df[!df$mout=="YesMOut",]
# Define initial parameters:
p <-length(m4$coefficients)
n <- length(m4$fitted.values)
h_param <- 3

# Residual analysis:
llres <- which(abs(rstudent(m4))>3);llres
length(llres)
par(mfrow=c(1,1))
influencePlot(m4, id=list(n=10))
Boxplot(cooks.distance(m4),id=list(labels=row.names(dfwork)))

# A priori influential observation
ll_priori_influential <- which(abs(hatvalues(m4))>h_param*(p/n))
length(ll_priori_influential)

# A posteriori influential observation:
ll_posteriori_influential<-which(abs(cooks.distance(m4))>(4/(n-p)));length(ll_posteriori_influential)
ll_unique_influential<-unique(c(ll_priori_influential,ll_posteriori_influential));length(ll_unique_influential)

m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=dfwork[-ll_posteriori_influential,])
summary(m4)
```

We can see that the observation 1725 is clearly an outlier of regression.
We have 98 a priori influential observations.
We have 177 a posteriori influential observations.
Between the a priori influential observations and the a posteriori influential observations we have 226 unique observations.

To achieve the best model we remove the a posteriori influential observations from the model. 

# Binary/Logistic Regression Models
