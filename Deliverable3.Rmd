---
title: "USED CAR PRICES CASE STUDY"
subtitle: 'Deliverable 3: General and Binary/Logistic Regression Models'
author: "Miquel Parra i Xavier Alaman"
date: \today
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

# R libraries imports, useful functions and data loading

In this first section we will load all required packages and libraries, and load our data.

## Load Required Packages

```{r, message=FALSE, warning=FALSE, results='hide'}
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))

requiredPackages <- c("effects","FactoMineR","car", "factoextra","RColorBrewer","ggplot2","dplyr","ggmap","ggthemes","knitr")

#use this function to check if each package is on the local machine
#if a package is installed, it will be loaded
#if any are not, the missing package(s) will be installed and loaded
package.check <- lapply(requiredPackages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})
#verify they are loaded
search()

```

## Sample load
This sample has a new variable called engineSize_num that has three categories: small_engine (engineSize<=2), medium_engine (2<engineSize<=3) and large_engine (engineSize>3).
```{r, results='hide'}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

# Users file path
miquel_fp <- "C:/Users/Miquel/Documents/GitHub/ADEI/"
xavi_fp <- "~/Documents/FIB/ADEI/ADEI/"
filepath <- xavi_fp
filepath <- miquel_fp 
# Set working directory
setwd(filepath)

# Load data from file
load(paste0(filepath,"MyOldCars-5000Clean2.RData"))
# Index reset
row.names(df) <- NULL
```

## Preparing the data 
We cannot have values equal to zero in the variables because in case we apply a logarithmic transformation to them, this would give an error since the logarithm of zero is undefined.
```{r}
names(df)
summary(df$engineSize)
levels(df$engineSize)
vars_con<-names(df)[c(5,7,8,11)]
vars_res<-names(df)[c(3,19)]
vars_dis<-names(df)[c(1,2,4,6,9,10,12,14,15,16,17,18)]
ll<-which(df$years_sell==0);ll
df$years_sell[ll]<-0.5

ll<-which(df$tax==0);ll
df$tax[ll]<-0.5

ll<-which(df$mileage==0);ll
df$mileage[ll]<-0.5

ll<-which(df$mpg==0);ll
df$mpg[ll]<-0.5
```

# Linear Models
We do linear regression in order to predict the value of price variable based on/according to the values of other variables.

## Only Numeric variables
```{r}
m0<-lm(price~1,data=df)
m1<-lm(price~mileage+tax+mpg+years_sell,data=df)
anova(m0,m1)
summary(m1)
vif(m1)
par(mfrow=c(2,2))
plot(m1,id.n=0)
residualPlots(m1,id=list(method=cooks.distance(m1),n=10)) 
```

As we can see some or all of the regressors are useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.

As we can see all the chosen variables have coefficients different from zero, i.e. they are useful, because their p-values are less than 0.05.
These explanatory variables explain a 52.4 % of the target's variability. 
Also, we can see that there are no collinear variables, i.e., highly correlated variables.

However, we can see that residuals are neither homoscedastic nor normal. So, we have to apply transformations. Also, the clearest nonlinear variable is mpg. 

### Target transformation
```{r}
library(MASS)
# Target variable transformation?
par(mfrow=c(1,1))
boxcox(price~mileage+tax+mpg+years_sell,data=df)
# Lambda=0 - log transformation is needed

# New model:
m2<-lm(log(price)~mileage+tax+mpg+years_sell,data=df)
summary(m2)
vif(m2) 
par(mfrow=c(2,2))
plot(m2, id.n=0)
residualPlots(m2,id=list(method=cooks.distance(m2),n=10)) 
```

This model is better than the previous one.

As we can see the transformation needed is a logarithmic transformation, because λ ≈ 0. 

Now, we can see that tax variable is not useful, because its p-value is greater than 0.05, so we should remove it but we won't do it because applying a transformation to it we will make it useful. However, the explanatory variables explain a 61.18 % of the target's variability.
There are still no collinear variables because they have not changed. 

However, the residuals continue to be neither homoscedastic nor normal. So, more transformations are needed. Also, the clearest nonlinear variables are mpg and years_sell. 


### Explanatory variables transformation
```{r}
boxTidwell(log(price)~mileage+tax+mpg, data=df[!df$mout=="YesMOut",])
```

We need to cube the tax variable, take the square root of the mileage variable and take the square root and raise to minus one the mpg variable.

```{r}
m3<-lm(log(price)~sqrt(mileage)+poly(tax,3)+ I(mpg^(-1/2))+years_sell,data=df[!df$mout=="YesMOut",])

summary(m3)
vif(m3) 
par(mfrow=c(2,2))
plot(m3, id.n=0)
residualPlots(m3,id=list(method=cooks.distance(m3),n=10)) 
```

This model is better than the previous one. 

As we can see all the chosen variables have coefficients different from zero, i.e. they are useful, because their p-values are less than 0.05.

These explanatory variables explain a 56.51 % of the target's variability. 
We can say that the mileage and years_sell variables are collinear, i.e., they are highly correlated. However, this is because we have adapted them.

Also, we can see that residuals are now homoscedastic and normal. However, we can see some influential observation. Also, there are no excessively non-linear variables.

## Including factors

### Significant factors
```{r}
m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
m4pet<-update(m3, ~.+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m4pet,m4)
```
As we can see transmission variable is useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better. 

```{r}
m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
m4pet<-update(m3, ~.+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m4pet,m4)
```
As we can see fuelType variable is useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better. 

```{r}
m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
m4pet<-update(m3, ~.+transmission+fuelType+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m4pet,m4)
```
As we can see manufacturer variable is useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better. 

```{r}
m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
m4pet<-update(m3, ~.+transmission+fuelType+manufacturer,data=df[!df$mout=="YesMOut",])
anova(m4pet,m4)
```
As we can see engineSize_num variable is useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.

```{r}
m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m3, m4)
summary(m4)
vif(m4)
par(mfrow=c(2,2))
plot(m4)
marginalModelPlots(m4)
#residualPlots(m4,id=list(method=cooks.distance(m4),n=10)) 
plot(allEffects(m4), selection = 5)
plot(allEffects(m4), selection = 6)
plot(allEffects(m4), selection = 7)
plot(allEffects(m4), selection = 8)

```

As we can see some or all of the new regressors are useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.

These explanatory variables explain an 83.86 % of the target's variability. 

We can say that the mileage and years_sell variables are still collinear, i.e., they are highly correlated. However, this is because we have adapted them.

Also, we can see that residuals are still homoscedastic and normal. However, we can see some influential observation. 

Also, we can see that the model captures the data well.

We can see:
The fact that a car is semi-automatic makes the logarithm of the price increase by 0.17 units. 
The fact that a car is automatic makes the logarithm of the price increase by 0.15 units. 
The effect of being a hybrid car, an other car or a diesel car (baseline) is the same, because their p-values are greater than 0.05.
The fact that a car is petrol makes the logarithm of the price decrease by 0.24 units. 
The fact that a car is BWM makes the logarithm of the price decrease by 0.045 units.
The effect of being a Mercedes car and an Audi car (baseline) is the same, because its p-value is greater than 0.05.
The effect that a car is VW makes the logarithm of the price decrease by 0.21 units.
The effect that a car have medium_engine makes the logarithm of the price decrease by 0.19 units.
The effect that a car have small_engine makes the logarithm of the price decrease by 0.29 units.

## Interactions
### Factors interaction
We are going to see if price variable (Y response) is related to fuelType (factor A) and aux_tax (factor B) variables.
```{r}
m5<-lm(log(price)~sqrt(mileage)+fuelType*aux_tax+poly(tax,3)+I(mpg^(-1/2))+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m4,m5)
AIC(m4,m5)
summary(m5)
par(mfrow=c(2,2))
plot(m5)
plot(allEffects(m5), selection = 8)
```
As we can see price variable is related to fuelType and aux_tax variables, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.

These explanatory variables explain a 83.97 % of the target's variability. 
Also, we can see that residuals are still homoscedastic and normal. However, we can see some influential observation.

We can see:
The fact that a car is hybrid and with tax between 125 and 145 make the logarithm of the price decrease by (-0.037-0.19+0.049) 0.18 units. 
The fact that a car is other and with tax between 125 and 145 make the logarithm of the price decrease by 0.24 units.
The fact that a car is petrol and with tax between 125 and 145 make the logarithm of the price decrease by 0.44 units. 
The fact that a car is petrol and with tax between 145 and 580 make the logarithm of the price decrease by 0.45 units.

### Factor and covariate interaction
We are going to see if depending on the fuelType and its mpg the price changes/varies.
We are going to see if price variable (Y response) is related to fuelType (factor A) and mpg (numeric) variables.

//ALTERNATIVA 1
```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*aux_tax+fuelType*mpg+poly(tax,3)+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m5,m6)
summary(m6)

```

As we can see price variable is related to fuelType and mpg variables, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.

These explanatory variables explain a 84.67 % of the target's variability. 

We can see that being within/in the the hybrid category and diesel category (baseline) is the same, because the p-value is greater than 0.05.
We can see that being within/in the the other category and diesel category (baseline) is the same, because the p-value is greater than 0.05.
We can see that within/in the petrol category, the mpg variable causes the logarithm of the price increase by (0.099-0.015-0.0068) 0.077 units.

(EN CAS D'EXPLICAR-LES LES DOS INTERACCIONS JUNTES I A DALT NOMES DIR Q ES SIGNIFICATIVA)

The fact that a car is hybrid and with tax between 125 and 145 make the logarithm of the price decrease by (-0.38+0.063+0.045) 0.27 units. 
The fact that a car is other and with tax between 125 and 145 make the logarithm of the price decrease by (-0.36+0.063+0.13) 0.17 units.
The fact that a car is petrol and with tax between 125 and 145 make the logarithm of the price increase by (0.099+0.063-0.027) 0.14 units. 
The fact that a car is petrol and with tax between 145 and 580 make the logarithm of the price increase by (0.099+0.029+0.033) 0.16 units.

//ALTERNATIVA 2
```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*aux_tax+fuelType*I(mpg^(-1/2))+poly(tax,3)+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m5,m6)
summary(m6)
```
As we can see price variable is related to fuelType and mpg variables, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.
We can see that within/in the hybrid category, the mpg variable causes the logarithm of the price decrease by (1.24+1.11-9.45) 7.1 units.

These explanatory variables explain a 84.52 % of the target's variability. 

We can see that being within/in the the other category and diesel category (baseline) is the same, because the p-value is greater than 0.05.
We can see that within/in the petrol category, the mpg variable causes the logarithm of the price increase by 2.18 units.

(EN CAS D'EXPLICAR-LES LES DOS INTERACCIONS JUNTES I A DALT NOMES DIR Q ES SIGNIFICATIVA)

The fact that a car is hybrid and with tax between 125 and 145 make the logarithm of the price increase by (1.24-0.15+0.11) 1.2 units. 
The fact that a car is other and with tax between 125 and 145 make the logarithm of the price increase by (0.47-0.15+0.13) 0.45 units.
The fact that a car is petrol and with tax between 125 and 145 make the logarithm of the price decrease by (-0.46-0.15-0.018) 0.63 units. 
The fact that a car is petrol and with tax between 145 and 580 make the logarithm of the price decrease by (-0.46-0.20+0.036) 0.62 units.

## Best model selection
//ALTERNATIVA 1
```{r}
m7 <- step( m6, k=log(nrow(df[!df$mout=="YesMOut",])))
summary(m7)
par(mfrow=c(2,2))
plot(m7)
```
Our best model explains (model that explains the maximum possible variability of the target with/using the least number of possible variables) a 84.62 % of the target's variability.

Also, we can see that residuals are still homoscedastic and normal. However, we can see some influential observation. 

//ALTERNATIVA 2
```{r}
m7 <- step( m6, k=log(nrow(df[!df$mout=="YesMOut",])))
summary(m7)
par(mfrow=c(2,2))
plot(m7)
```
Our best model (model that explains the maximum possible variability of the target with/using the least number of possible variables) explains a 84.41 % of the target's variability. 

Also, we can see that residuals are still homoscedastic and normal. However, we can see some influential observation. 

## Diagnostics
//ALTERNATIVA 1
A good model should be consistent with theoretical properties in residual analysis. Neither influential
nor unusual data should be included.
```{r}
dfwork <- df[!df$mout=="YesMOut",]
# Define initial parameters:
p <-length(m7$coefficients)
n <- length(m7$fitted.values)
h_param <- 3

# Residual analysis:
llres <- which(abs(rstudent(m7))>3);llres
length(llres)
par(mfrow=c(1,1))
influencePlot(m7, id=list(n=10))
Boxplot(cooks.distance(m7),id=list(labels=row.names(dfwork)))

# A priori influential observation
ll_priori_influential <- which(abs(hatvalues(m7))>h_param*(p/n))
length(ll_priori_influential)

# A posteriori influential observation:
ll_posteriori_influential<-which(abs(cooks.distance(m7))>(4/(n-p)));length(ll_posteriori_influential)
ll_unique_influential<-unique(c(ll_priori_influential,ll_posteriori_influential));length(ll_unique_influential)

m7 <- update(m7,data=dfwork[-ll_posteriori_influential,])
summary(m7)
par(mfrow=c(2,2))
plot(m7)
```
We have 30 outliers on the regression. 
We have 63 a priori influential observations.
We have 122 a posteriori influential observations. As we can see in the Boxplot the 4853 and 1725 observations are the most significant ones. 
Between the a priori influential observations and the a posteriori influential observations we have 151 unique observations.That means that we have 34 observations that were influential a priori and then beacame influential a posteriori

To achieve the best model we remove the a posteriori influential observations from the model.

These explanatory variables explain a 86.13 % of the target's variability. 
Also, we can see that residuals are still homoscedastic and normal. 

//ALTERNATIVA 2
A good model should be consistent with theoretical properties in residual analysis. Neither influential
nor unusual data should be included.
```{r}
dfwork <- df[!df$mout=="YesMOut",]
# Define initial parameters:
p <-length(m7$coefficients)
n <- length(m7$fitted.values)
h_param <- 3

# Residual analysis:
llres <- which(abs(rstudent(m7))>3);llres
length(llres)
par(mfrow=c(1,1))
influencePlot(m7, id=list(n=10))
Boxplot(cooks.distance(m7),id=list(labels=row.names(dfwork)))

# A priori influential observation
ll_priori_influential <- which(abs(hatvalues(m7))>h_param*(p/n))
length(ll_priori_influential)

# A posteriori influential observation:
ll_posteriori_influential<-which(abs(cooks.distance(m7))>(4/(n-p)));length(ll_posteriori_influential)
ll_unique_influential<-unique(c(ll_priori_influential,ll_posteriori_influential));length(ll_unique_influential)

m7 <- update(m7,data=dfwork[-ll_posteriori_influential,])
summary(m7)
par(mfrow=c(2,2))
plot(m7)
```
We have 30 outliers on the regression. 
We have 97 a priori influential observations.
We have 161 a posteriori influential observations. As we can see in the Boxplot the 1725 and 4853 observations are the most significant ones. 
Between the a priori influential observations and the a posteriori influential observations we have 209 unique observations.That means that we have 49 observations that were influential a priori and then beacame influential a posteriori

To achieve the best model we remove the a posteriori influential observations from the model. 

These explanatory variables explain a 86.43 % of the target's variability. 
Also, we can see that residuals are still homoscedastic and normal. 

# Binary/Logistic Regression Models

## Dividing/Splitting the sample
```{r}
set.seed(1234)
llwork <- sample(1:nrow(df),round(0.70*nrow(df),0))


df_train <- df[llwork,]
df_test <-df[-llwork,]
```

## Only numeric variables
```{r}
m0<-glm(Audi~1,family="binomial",data=df_train[!df_train$mout=="YesMOut",])
m1<-glm(Audi~mileage+tax+mpg+years_sell,family="binomial",data=df_train[!df_train$mout=="YesMOut",])
anova( m0, m1, test="Chisq")
summary(m1)
vif(m1) 
marginalModelPlots(m1)
```

As we can see the decrease in deviance is significant, because the p-value of anova test is less than 0.05.

We can say that against more mpg less probability of being Audi. More specifically, increasing by 1 unit mpg then exp(-0.028*1) = 0.97 -> 100*(1-0.97) = 3 %, the probability of being Audi decreases by 3 %.    

As we can see the only useful variable is the mpg variable, because its p-value is less than 0.05. We should remove the other variables but we won't do it because applying transformations to them or adding the factors they may become significant as well.  

Regarding correlations between variables we don't have to worry (because there aren't >=4). Also, we can see that the model captures the data well.

### Transformations
```{r}
m2<-glm(Audi~sqrt(mileage)+poly(tax,3)+mpg+years_sell,family="binomial",data=df_train[!df_train$mout=="YesMOut",])
summary(m2)
marginalModelPlots(m2)
```
Trying the same transformations that in the previous model (price as target), except the mpg transformation, we can see that now the useful variables are mpg, years_sell and tax. However, in the marginalModelPlots we can see that this model captures the data worse (bottom left plot), so we have decided not to apply transformations on the regressors. 

## Including factors

```{r}
m2 <- update(m1, ~.+fuelType+transmission+engineSize_num,data=df_train[!df_train$mout=="YesMOut",])
m2pet <- update(m1, ~.+transmission+engineSize_num,data=df_train[!df_train$mout=="YesMOut",])
anova( m2pet, m2, test="Chisq")
```
As we can see fuelType variable is useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better. 

```{r}
m2 <- update(m1, ~.+fuelType+transmission+engineSize_num,data=df_train[!df_train$mout=="YesMOut",])
m2pet <- update(m1, ~.+fuelType+engineSize_num,data=df_train[!df_train$mout=="YesMOut",])
anova( m2pet, m2, test="Chisq")
```
As we can see transmission variable is useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better. 

```{r}
m2 <- update(m1, ~.+fuelType+transmission+engineSize_num,data=df_train[!df_train$mout=="YesMOut",])
m2pet <- update(m1, ~.+fuelType+transmission,data=df_train[!df_train$mout=="YesMOut",])
anova( m2pet, m2, test="Chisq")
```
As we can see engineSize_num variable is useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better. 

```{r}
m2 <- update(m1, ~.+fuelType+transmission+engineSize_num,data=df_train[!df_train$mout=="YesMOut",])
anova( m1, m2, test="Chisq")
summary(m2)
marginalModelPlots(m2)

```

As we can see the decrease in deviance is significant, because the p-value of anova test is less than 0.05.

We can see:
Being hybrid the probability of being Audi decreases by (exp(-1.66) = 0.19 -> 100*(1-0.19) = 81 %) 81 %.
Being other the probability of being Audi decreases by 100 %.
Being petrol the probability of being Audi decreases by 29.5 %.
Being semiAuto the probability of being Audi decreases by 26.7 %.
Being automatic the probability of being Audi decreases by 11.3 %.
Being medium_engine the probability of being Audi increases by 78.6 %.
Being small_engine the probability of being Audi increases by 493 %. 

Also, we can see that the model captures the data well.

## Interactions
### Factors interaction
We are going to see if Audi variable (Y response) is related to fuelType (factor A) and transmission (factor B) variables.
//ALTERNATIVA 1
```{r}
m3 <- update(m1, ~.+fuelType:transmission+engineSize_num,data=df_train[!df_train$mout=="YesMOut",])
anova(m2,m3,test="Chisq")
summary(m3)
```
As we can see the decrease in deviance is significant, because the p-value of anova test is less than 0.05, so we can say that Audi is related to fuelType and transmission.

We can see:
The fact that a car is diesel and with manual transmission makes the probability of being Audi increase by (exp(0.66) = 1.93 -> 100*(1.93-1) = 93 %) increase by 93 %. 
The fact that a car is other and with manual transmission makes the probability of being Audi decrease by 100 %.
The fact that a car is petrol and with manual transmission makes the probability of being Audi decrease by 8.8 %.
The fact that a car is diesel and with semiAuto transmission makes the probability of being Audi decrease by 8.4 %. 
The fact that a car is hybrid and with semiAuto transmission makes the probability of being Audi decrease by 42.9 %.
The fact that a car is petrol and with semiAuto transmission makes the probability of being Audi decrease by 2.2 %.
The fact that a car is diesel and with automatic transmission makes the probability of being Audi increase by 32.3 %.
The fact that a car is hybrid and with automatic transmission makes the probability of being Audi decrease by 100 %
The fact that a car is other and with automatic transmission makes the probability of being Audi decrease by 100 %.

//ALTERNATIVA 2
```{r}
m3 <- update(m1, ~.+fuelType*transmission+engineSize_num,data=df_train[!df_train$mout=="YesMOut",])
anova(m2,m3,test="Chisq")
summary(m3)
```
As we can see the decrease in deviance is significant, because the p-value of anova test is less than 0.05, so we can say that Audi is related to fuelType and transmission.

We can see:
The fact that a car is diesel and with manual transmission makes the probability of being Audi increase by (exp(0.66) = 1.93 -> 100*(1.93-1) = 93 %) increase by 93 %. 
The fact that a car is hybrid and with semiAuto transmission makes the probability of being Audi decrease by 70.5 %. 
The fact that a car is petrol and with semiAuto transmission makes the probability of being Audi decrease by (exp(-0.75-0.75+0.81) = 0.5 -> 100*(1-0.5) = 50 %) decrease by 50 %.
The fact that a car is other and with automatic transmission makes the probability of being Audi decrease by 100 %
The fact that a car is petrol and with automatic transmission makes the probability of being Audi decrease by 48 %.

### Factor and covariate interaction
//ALTERNATIVA 1
```{r}
m3 <- update(m1, ~.+fuelType:transmission+fuelType:mpg+engineSize_num,data=df_train[!df_train$mout=="YesMOut",])
anova(m2,m3,test="Chisq")
summary(m3)
```
As we can see the decrease in deviance is significant, because the p-value of anova test is less than 0.05, so we can say that Audi is related to fuelType and mpg.

We can see:
Within/in the hybrid category, the mpg variable causes the probability of being Audi increase by
Within/in the other cateogry, the mpg variable causes the probability of being Audi increase by
Within/in the petrol category, the mpg variable causes the probability of being Audi increase by (exp(0.022) = 1.02 -> 100*(1.02-1) = 2 %) 2 %.


The fact that a car is diesel and with manual transmission makes the probability of being Audi increase by (exp(1.77) = 1.93 -> 100*(1.93-1) = 93 %) increase by 93 %. 
The fact that a car is diesel and with automatic transmission makes the probability of being Audi increase by (exp(1.32) = 1.93 -> 100*(1.93-1) = 93 %) increase by 93 %.


//ALTERNATIVA 2
```{r}
m3 <- glm(Audi~mileage+tax+years_sell+fuelType*transmission+fuelType*mpg+engineSize_num,family="binomial",data=df_train[!df_train$mout=="YesMOut",])
anova(m2,m3,test="Chisq")
summary(m3)
```
As we can see the decrease in deviance is significant, because the p-value of anova test is less than 0.05, so we can say that Audi is related to fuelType and mpg.

We can see: 
Within/in the hybrid category, the mpg variable causes the probability of being Audi decrease by 100 %.
Within/in the other category, the mpg variable cause the probability of being Audi decrease by 100 %.
Within/in the petrol category, the mpg variable causes the probability of being Audi decrease by (exp(-1.98-0.057+0.022) = 0.13 -> 100*(1 - 0.13) = 87 %) 87 %.

(EN CAS D'EXPLICAR-LES LES DOS INTERACCIONS JUNTES I A DALT NOMES DIR Q ES SIGNIFICATIVA)

The fact that a car is hybrid with semiAuto transmission makes the probability of being Audi decrease by 100 %.
The fact that a car is petrol with semiAuto transmission makes the probability of being Audi decrease by (exp(-1.98-0.82+1) = 0.5 -> 100*(1-0.5) = 50 %) decrease by 50 %.
The fact that a car is other with automatic transmission makes the probability of being Audi decrease by 100 %.
The fact that a car is petrol with automatic transmission makes the probability of being Audi decrease by 83 %.

## Best model selection
//ALTERNATIVA 1
```{r}
m4 <- step(m3) 
summary(m4)
```
Our best model (model that has the minimum possible deviance with/using the least number of possible variables) has a deviance of 3116.5. 

//ALTERNATIVA 2
```{r}
m4 <- step(m3) 
summary(m4)
```
Our best model (model that has the minimum possible deviance with/using the least number of possible variables) has a deviance of 3116.5. 

## Diagnostics
//ALTERNATIVA 1
```{r}
dfwork <- df_train[!df_train$mout=="YesMOut",]

Boxplot(abs(rstudent(m4)))
llres <- which(abs(rstudent(m4))>2.3);llres
length(llres)
Boxplot(hatvalues(m4),id=list(labels=row.names(dfwork)))
influencePlot(m4, id=list(n=10))
Boxplot(cooks.distance(m4),id=list(labels=row.names(dfwork)))
llout<-which(abs(cooks.distance(m4))>0.02);
length(llout)
llrem<-unique(c(llout,llres));llrem

#m4<-glm(Audi ~ mpg + years_sell + fuelType + transmission + engineSize_num, family="binomial",data=dfwork[-llrem,])
m4 <- update(m4,data=dfwork[-llrem,])
summary(m4)
m0<-glm(Audi ~ 1, family="binomial", data=dfwork[-llrem,])

```
We have 3 outliers on the regression. 
We have 4 a posteriori influential observations. As we can see in the Boxplot the 4487 observation is the most significant one. 

To achieve the best model we remove the a posteriori influential observations from the model. 

These model has a deviance of 3088.5.

//ALTERNATIVA 2
```{r}
dfwork <- df_train[!df_train$mout=="YesMOut",]

Boxplot(abs(rstudent(m4)))
llres <- which(abs(rstudent(m4))>2.3);llres
length(llres)
Boxplot(hatvalues(m4),id=list(labels=row.names(dfwork)))
influencePlot(m4, id=list(n=10))
Boxplot(cooks.distance(m4),id=list(labels=row.names(dfwork)))
llout<-which(abs(cooks.distance(m4))>0.02);
length(llout)
llrem<-unique(c(llout,llres));llrem

#m4<-glm(Audi ~ mpg + years_sell + fuelType + transmission + engineSize_num, family="binomial",data=dfwork[-llrem,])
m4 <- update(m4,data=dfwork[-llrem,])
summary(m4)
m0<-glm(Audi ~ 1, family="binomial", data=dfwork[-llrem,])

```
We have 2 outliers on the regression. 
We have 3 a posteriori influential observations. As we can see in the Boxplot the 515, 453 and 469 observations are the most significant ones. 

To achieve the best model we remove the a posteriori influential observations from the model. 

These model has a deviance of 3088.5.

## Prediction
```{r}
#pred_test <- predict(m4, newdata=df_test, type="response")
pred_test
library("ROCR")
library("AUC")

dadesroc<-prediction(pred_test,df_test$Audi)
par(mfrow=c(1,2))
performance(dadesroc,"auc",fpr.stop=0.05)
plot(performance(dadesroc,"err"))
plot(performance(dadesroc,"tpr","fpr"))
abline(0,1,lty=2)
roc(pred_test,df_test$Audi)
```

## Confusion matrix
```{r}
treshold <- 0.5
audi.est <- ifelse(pred_test<treshold,0,1)
audi.est
tt<-table(audi.est,df_test$Audi);tt
100*sum(diag(tt))/sum(tt)
# Model na?ve
prob.audi <- m0$fit
audi.est <- ifelse(prob.audi<0.5,0,1)
tt<-table(audi.est,dfwork$Audi[-llrem]);tt
100*tt[1,1]/sum(tt)
```


