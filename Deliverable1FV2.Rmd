---
title: "USED CAR PRICES CASE STUDY"
subtitle: 'Deliverable 3: General and Binary/Logistic Regression Models'
author: "Miquel Parra i Xavier Alaman"
date: \today
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

# R libraries imports, useful functions and data loading

In this first section we will load all required packages and libraries, and load our data.

## Load Required Packages

```{r, message=FALSE, warning=FALSE, results='hide'}
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))

requiredPackages <- c("effects","FactoMineR","car", "factoextra","RColorBrewer","ggplot2","dplyr","ggmap","ggthemes","knitr")

#use this function to check if each package is on the local machine
#if a package is installed, it will be loaded
#if any are not, the missing package(s) will be installed and loaded
package.check <- lapply(requiredPackages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})
#verify they are loaded
search()

```

## Sample load

```{r, results='hide'}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

# Users file path
miquel_fp <- "C:/Users/Miquel/Documents/GitHub/ADEI/"
xavi_fp <- "~/Documents/FIB/ADEI/ADEI/"
filepath <- xavi_fp
filepath <- miquel_fp 
# Set working directory
setwd(filepath)

# Load data from file
load(paste0(filepath,"MyOldCars-5000Clean2.RData"))
# Index reset
row.names(df) <- NULL
```

## Preparing the data 
We cannot have values equal to zero in the variables because in case we apply a logarithmic transformation to them, this would give an error since the logarithm of zero is undefined.
```{r}
names(df)
summary(df$engineSize)
levels(df$engineSize)
vars_con<-names(df)[c(5,7,8,11)]
vars_res<-names(df)[c(3,19)]
vars_dis<-names(df)[c(1,2,4,6,9,10,12,14,15,16,17,18)]
ll<-which(df$years_sell==0);ll
df$years_sell[ll]<-0.5

ll<-which(df$tax==0);ll
df$tax[ll]<-0.5

ll<-which(df$mileage==0);ll
df$mileage[ll]<-0.5

ll<-which(df$mpg==0);ll
df$mpg[ll]<-0.5
```

# Linear Models
We do linear regression in order to predict the value of price variable based on/according to the values of other variables.

## Only Numeric variables
```{r}
m0<-lm(price~1,data=df)
m1<-lm(price~mileage+tax+mpg+years_sell,data=df)
anova(m0,m1)
summary(m1)
vif(m1)
par(mfrow=c(2,2))
plot(m1,id.n=0)
residualPlots(m1,id=list(method=cooks.distance(m1),n=10)) 
```

As we can see mileage, tax, mpg and years_sell variables are useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.

As we can see all the chosen variables have coefficients different from zero, i.e. they are useful, because their p-values are less than 0.05.
These explanatory variables explain a 52.4 % of the target's variability. 
Also, we can see that there are no collinear variables, i.e., highly correlated variables.

However, we can see that residuals are neither homoscedastic nor normal. So, we have to apply transformations. Also, the clearest nonlinear variable is mpg. 

### Target transformation
```{r}
library(MASS)
# Target variable transformation?
par(mfrow=c(1,1))
boxcox(price~mileage+tax+mpg+years_sell,data=df)
# Lambda=0 - log transformation is needed

# New model:
m2<-lm(log(price)~mileage+tax+mpg+years_sell,data=df)
summary(m2)
vif(m2) 
par(mfrow=c(2,2))
plot(m2, id.n=0)
residualPlots(m2,id=list(method=cooks.distance(m2),n=10)) 
```

This model is better than the previous one.

As we can see the transformation needed is a logarithmic transformation, because λ ≈ 0. 

Now, we can see that tax variable is not useful, because its p-value is greater than 0.05, so we should remove it but we won't do it because applying a transformation to it we will make it useful. However, the explanatory variables explain a 61.18 % of the target's variability.
There are still no collinear variables because they have not changed. 

However, the residuals continue to be neither homoscedastic nor normal. So, more/further transformations are needed/required. Also, the clearest nonlinear variables are mpg and years_sell. 


### Explanatory variables transformation
```{r}
boxTidwell(log(price)~mileage+tax+mpg, data=df[!df$mout=="YesMOut",])
#?boxTidwell
```

We need to cube the tax variable, take the square root of the mileage variable and take the square root and raise to minus one the mpg variable.

```{r}
#df2 <- df[!df$mout=="YesMOut",]

m3<-lm(log(price)~sqrt(mileage)+poly(tax,3)+ I(mpg^(-1/2))+years_sell,data=df[!df$mout=="YesMOut",])

summary(m3)
vif(m3) 
par(mfrow=c(2,2))
plot(m3, id.n=0)
residualPlots(m3,id=list(method=cooks.distance(m3),n=10)) 
```

This model is better than the previous one. 

As we can see all the chosen variables have coefficients different from zero, i.e. they are useful, because their p-values are less than 0.05.

These explanatory variables explain a 56.51 % of the target's variability. 
We can say that the mileage and years_sell variables are collinear, i.e., they are highly correlated. However, this is because we have adapted them.

Also, we can see that residuals are now homoscedastic and normal. However, we can see some influential observation. Also, there are no excessively non-linear variables

## Including factors

### Significant factors
```{r}
m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
m4pet<-update(m3, ~.+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m4pet,m4)
```
As we can see transmission variable is useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better. 

```{r}
m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
m4pet<-update(m3, ~.+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m4pet,m4)
```
As we can see fuelType variable is useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better. 

```{r}
m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
m4pet<-update(m3, ~.+transmission+fuelType+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m4pet,m4)
```
As we can see manufacturer variable is useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better. 

```{r}
m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
m4pet<-update(m3, ~.+transmission+fuelType+manufacturer,data=df[!df$mout=="YesMOut",])
anova(m4pet,m4)
```
As we can see engineSize_num variable is useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.

```{r}
m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m3, m4)
summary(m4)
vif(m4)
#acf(rstudent(m4))
par(mfrow=c(2,2))
plot(m4)
marginalModelPlots(m4)
#residualPlots(m4,id=list(method=cooks.distance(m4),n=10)) 
plot(allEffects(m4), selection = 5)
plot(allEffects(m4), selection = 6)
plot(allEffects(m4), selection = 7)
plot(allEffects(m4), selection = 8)

```

As we can see transmission, fuelType, manufacturer and engineSize_num are useful, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.

These explanatory variables explain an 83.86 % of the target's variability. 

We can say that the mileage and years_sell variables are still collinear, i.e., they are highly correlated. However, this is because we have adapted them.
Also, we can see that residuals are still homoscedastic and normal. However, we can see some influential observation. 
Also, we can see that the model captures the data well.

We can see:
The fact that a car is semi-automatic makes the logarithm of the price increase by 0.17 units. 
The fact that a car is automatic makes the logarithm of the price increase by 0.15 units. 
The effect of being a hybrid car, an other car or a diesel car (baseline) is the same, because their p-values are greater than 0.05.
The fact that a car is petrol makes the logarithm of the price decrease by 0.24 units. 
The fact that a car is BWM makes the logarithm of the price decrease by 0.045 units.
The effect of being a Mercedes car and an Audi car (baseline) is the same, because its p-value is greater than 0.05.
The effect that a car is VW makes the logarithm of the price decrease by 0.21 units.
The effect that a car have medium_engine makes the logarithm of the price decrease by 0.19 units.
The effect that a car have small_engine makes the logarithm of the price decrease by 0.29 units.

## Interactions
### Factors interaction
We are going to see if price variable (Y response) is related to fuelType (factor A) and aux_tax (factor B) variables.
```{r}
m5<-lm(log(price)~sqrt(mileage)+fuelType*aux_tax+poly(tax,3)+I(mpg^(-1/2))+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m4,m5)
AIC(m4,m5)
summary(m5)
par(mfrow=c(2,2))
plot(m5)
plot(allEffects(m5), selection = 8)
```
As we can see price variable is related to fuelType and aux_tax variables, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.
These explanatory variables explain a 83.97 % of the target's variability. 
Also, we can see that residuals are still homoscedastic and normal. However, we can see some influential observation. 
We can see:
The fact that a car is hybrid and with tax between 125 and 145 make the logarithm of the price decrease by 0.18 units. 

The fact that a car is other and with tax between 125 and 145 make the logarithm of the price decrease by 0.24 units.

The fact that a car is petrol and with tax between 125 and 145 make the logarithm of the price decrease by 0.44 units. 

The fact that a car is petrol and with tax between 145 and 580 make the logarithm of the price decrease by 0.45 units.


//SOBRAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
```{r}
m5<-lm(log(price)~sqrt(mileage)+fuelType*aux_tax+poly(tax,3)+I(mpg^(-1/2))+years_sell+transmission,data=df[!df$mout=="YesMOut",])
summary(m5)
plot(allEffects(m5), selection = 6)
```
We can say that the price doesn't changes/varies depending on the fuelType and its aux_tax, because their p-values are greater than 0.05.


### Factor and covariate interaction
We are going to see if depending on the fuelType and its mpg the price changes/varies.
We are going to see if price variable (Y response) is related to fuelType (factor A) and mpg (numeric) variables.
```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*aux_tax+fuelType*I(mpg^(-1/2))+poly(tax,3)+I(mpg^(-1/2))+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
summary(m6)
anova(m5,m6)
plot(allEffects(m6), selection = 5)
```
As we can see price variable is related to fuelType and mpg variables, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.
We can see that within/in the hybrid category, the mpg variable causes the logarithm of the price decrease by 9.45 units.
We can see that being within/in the the other category and diesel category (baseline) is the same, because the p-value is greater than 0.05.
We can see that within/in the petrol category, the mpg variable causes the logarithm of the price increase by 1.53 units.

```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*aux_tax+fuelType*mpg+poly(tax,3)+I(mpg^(-1/2))+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
summary(m6)
anova(m5,m6)
plot(allEffects(m6), selection = 7)
```
As we can see price variable is related to fuelType and mpg variables, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.
We can see that within/in the hybrid category, the mpg variable causes the logarithm of the price increase by 0.011 units.
We can see that being within/in the the other category and diesel category (baseline) is the same, because the p-value is greater than 0.05.
We can see that within/in the petrol category, the mpg variable causes the logarithm of the price decrease by 0.0058 units.

```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*aux_tax+fuelType*mpg+poly(tax,3)+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m5,m6)
summary(m6)

```

As we can see price variable is related to fuelType and mpg variables, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.
We can see that being within/in the the hybrid category and diesel category (baseline) is the same, because the p-value is greater than 0.05.
We can see that being within/in the the other category and diesel category (baseline) is the same, because the p-value is greater than 0.05.
We can see that within/in the petrol category, the mpg variable causes the logarithm of the price decrease by 0.0068 units.

```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*aux_tax+fuelType*I(mpg^(-1/2))+poly(tax,3)+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
anova(m5,m6)
summary(m6)
```
As we can see price variable is related to fuelType and mpg variables, because the p-value of anova test is less than 0.05, that means that the 2 models are not equivalent and, therefore, the big one is better.
We can see that within/in the hybrid category, the mpg variable causes the logarithm of the price decrease by 9.45 units.
We can see that being within/in the the other category and diesel category (baseline) is the same, because the p-value is greater than 0.05.
We can see that within/in the petrol category, the mpg variable causes the logarithm of the price increase by 1.53 units.

//SOBRAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA
```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*I(mpg^(-1/2))+poly(tax,3)+years_sell+transmission+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])
summary(m6)
plot(allEffects(m6), selection = 7)
```
We can see:
The fact that a car is hybrid and with its mpg makes the logarithm of the price decrease by 9.52 units.
The effect of being a other car with its mpg and a diesel car with its mpg (baseline) is the same, because its p-value is greater than 0.05.
The fact that a car is petrol and with its mpg makes the logarithm of the price increase by 1.36 units.

```{r}
m6<-lm(log(price)~sqrt(mileage)+fuelType*I(mpg^(-1/2))+poly(tax,3)+years_sell+transmission,data=df[!df$mout=="YesMOut",])
summary(m6)
plot(allEffects(m6), selection = 5)
```
We can see:
The fact that a car is hybrid and with its mpg makes the logarithm of the price decrease by 9.64 units.
The effect of being a other car with its mpg and a diesel car with its mpg (baseline) is the same, because its p-value is greater than 0.05.
The fact that a car is petrol and with its mpg makes the logarithm of the price increase by 3.55 units.



## Diagnostics
//CREC Q JA NO SERA EL MES BO I SOBRARAAAAAAA
A good model should be consistent with theoretical properties in residual analysis. Neither influential
nor unusual data should be included.
```{r}
dfwork <- df[!df$mout=="YesMOut",]
# Define initial parameters:
p <-length(m4$coefficients)
n <- length(m4$fitted.values)
h_param <- 3

# Residual analysis:
llres <- which(abs(rstudent(m4))>3);llres
length(llres)
par(mfrow=c(1,1))
influencePlot(m4, id=list(n=10))
Boxplot(cooks.distance(m4),id=list(labels=row.names(dfwork)))

# A priori influential observation
ll_priori_influential <- which(abs(hatvalues(m4))>h_param*(p/n))
length(ll_priori_influential)

# A posteriori influential observation:
ll_posteriori_influential<-which(abs(cooks.distance(m4))>(4/(n-p)));length(ll_posteriori_influential)
ll_unique_influential<-unique(c(ll_priori_influential,ll_posteriori_influential));length(ll_unique_influential)

m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=dfwork[-ll_posteriori_influential,])
summary(m4)
```
He have 24 outliers on the regression. As we can see in the Boxplot the 1725 observation is the most significant one. 
We have 98 a priori influential observations.
We have 177 a posteriori influential observations.
Between the a priori influential observations and the a posteriori influential observations we have 226 unique observations.That means that we have 49 observations that were influential a priori and then beacame influential a posteriori

To achieve the best model we remove the a posteriori influential observations from the model. 

A good model should be consistent with theoretical properties in residual analysis. Neither influential
nor unusual data should be included.
```{r}
dfwork <- df[!df$mout=="YesMOut",]
# Define initial parameters:
p <-length(m5$coefficients)
n <- length(m5$fitted.values)
h_param <- 3

# Residual analysis:
llres <- which(abs(rstudent(m5))>3);llres
length(llres)
par(mfrow=c(1,1))
influencePlot(m5, id=list(n=10))
Boxplot(cooks.distance(m5),id=list(labels=row.names(dfwork)))

# A priori influential observation
ll_priori_influential <- which(abs(hatvalues(m5))>h_param*(p/n))
length(ll_priori_influential)

# A posteriori influential observation:
ll_posteriori_influential<-which(abs(cooks.distance(m5))>(4/(n-p)));length(ll_posteriori_influential)
ll_unique_influential<-unique(c(ll_priori_influential,ll_posteriori_influential));length(ll_unique_influential)

m5 <- update(m3, ~.+transmission+fuelType*aux_tax+manufacturer+engineSize_num,data=dfwork[-ll_posteriori_influential,])
summary(m5)
par(mfrow=c(2,2))
plot(m5)
```
He have 27 outliers on the regression. As we can see in the Boxplot the 1725 observation is the most significant one. 
We have 98 a priori influential observations.
We have 158 a posteriori influential observations.
Between the a priori influential observations and the a posteriori influential observations we have 207 unique observations.That means that we have 49 observations that were influential a priori and then beacame influential a posteriori

To achieve the best model we remove the a posteriori influential observations from the model. 

These explanatory variables explain a 86.4 % of the target's variability. 
Also, we can see that residuals are still homoscedastic and normal. However, we can see some influential observation. 

# Binary/Logistic Regression Models

## Dividing/Splitting the sample
```{r}
set.seed(1234)
llwork <- sample(1:nrow(df),round(0.70*nrow(df),0))


df_train <- df[llwork,]
df_test <-df[-llwork,]
```

## Only numeric variables
```{r}
m0<-glm(Audi~1,family="binomial",data=df_train[!df_train$mout=="YesMOut",])
m1<-glm(Audi~mileage+tax+mpg+years_sell,family="binomial",data=df_train[!df_train$mout=="YesMOut",])
anova( m0, m1, test="LR")
summary(m1)
vif(m1) 
marginalModelPlots(m1)
```

As we can see the decrease in deviance is significant, because the p-value of anova test is less than 0.05.

As we can see the only useful variable is the mpg variable, because its p-value is less than 0.05. So, we can try some transformations on the regressors. 
Regarding correlations between variables we don't have to worry (because there aren't >=4). Also, we can see that the model captures the data well.

### Transformations
```{r}
m2<-glm(Audi~sqrt(mileage)+poly(tax,3)+mpg+years_sell,family="binomial",data=df_train[!df_train$mout=="YesMOut",])
summary(m2)
marginalModelPlots(m2)
```
Trying the same transformations that in the previous model (price as target), except the mpg transformation, we can see that now the useful variables are mpg, years_sell and tax. However, in the marginalModelPlots we can see that this model captures the data worse (bottom left plot), so we have decided not to apply transformations on the regressors. 

## Including factors
```{r}
m2 <- update(m1, ~.+fuelType+transmission+engineSize_num,data=df_train[!df_train$mout=="YesMOut",])
anova( m1, m2, test="LR")
summary(m2)
marginalModelPlots(m2)
```

As we can see the decrease in deviance is significant, because the p-value of anova test is less than 0.05

As we can see the useful variables are mpg, years_sell, fuelType and transmission, because their p-values are less than 0.05. 
Also, we can see that the model captures the data well.

## Interactions
### Factors interaction
We are going to see if price variable (Y response) is related to fuelType (factor A) and aux_tax (factor B) variables.
```{r}
m3 <- update(m1, ~.+fuelType*aux_tax+transmission+engineSize_num,data=df_train[!df_train$mout=="YesMOut",])
anova(m2,m3,test="LR")
summary(m3)
```

As we can see the decrease in deviance is not significant, because the p-value of anova test is greater than 0.05


### Factor and covariate interaction
```{r}

```

## Best model
```{r}
#m? <- step(m?) 
```

## Diagnostics
```{r}
dfwork <- df_train[!df_train$mout=="YesMOut",]
#Boxplot(abs(rstudent(m?)))
#llres <- which(abs(rstudent(m?))>2.3);llres
#Boxplot(hatvalues(m?),id=list(labels=row.names(dfwork)))
#influencePlot(m?, id=list(n=10))
#Boxplot(cooks.distance(m?),id=list(labels=row.names(dfwork)))
#llout<-which(abs(cooks.distance(m?))>0.02);length(llout)
#llrem<-unique(c(llout,llres));llrem
#data=dfwork[-llrem,]
#m0<-glm(Audi ~ 1, family="binomial", data=dfwork[-llrem,])

```


## Prediction
```{r}
#pred_test <- predict(m?, newdata=df_test, type="response")
pred_test
library("ROCR")
library("AUC")

dadesroc<-prediction(pred_test,df_test$Audi)
par(mfrow=c(1,2))
performance(dadesroc,"auc",fpr.stop=0.05)
plot(performance(dadesroc,"err"))
plot(performance(dadesroc,"tpr","fpr"))
abline(0,1,lty=2)
roc(pred_test,df_test$Audi)
```

## Confusion matrix
```{r}
treshold <- 0.5
audi.est <- ifelse(pred_test<treshold,0,1)
audi.est
tt<-table(audi.est,df_test$Audi);tt
100*sum(diag(tt))/sum(tt)
# Model na?ve
prob.audi <- m0$fit
audi.est <- ifelse(prob.audi<0.5,0,1)
tt<-table(audi.est,dfwork$Audi[-llrem]);tt
100*tt[1,1]/sum(tt)
```


