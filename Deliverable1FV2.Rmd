---
title: "USED CAR PRICES CASE STUDY"
subtitle: 'Deliverable 3: General and Binary/Logistic Regression Models'
author: "Miquel Parra i Xavier Alaman"
date: \today
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

# R libraries imports, useful functions and data loading

In this first section we will load all required packages and libraries, and load our data.

## Load Required Packages

```{r, message=FALSE, warning=FALSE, results='hide'}
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))

requiredPackages <- c("effects","FactoMineR","car", "factoextra","RColorBrewer","ggplot2","dplyr","ggmap","ggthemes","knitr")

#use this function to check if each package is on the local machine
#if a package is installed, it will be loaded
#if any are not, the missing package(s) will be installed and loaded
package.check <- lapply(requiredPackages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})
#verify they are loaded
search()

```

## Sample load

```{r, results='hide'}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

# Users file path
miquel_fp <- "C:/Users/Miquel/Documents/GitHub/ADEI/"
xavi_fp <- "~/Documents/FIB/ADEI/ADEI/"
filepath <- xavi_fp
filepath <- miquel_fp 

# Set working directory
setwd(filepath)

# Load data from file
load(paste0(filepath,"MyOldCars-5000Clean2.RData"))
levels(df$engineSize_num)
# Index reset
row.names(df) <- NULL
```

## Preparing the data 
We cannot have values equal to zero in the variables because in case we apply a logarithmic transformation to them, this would give an error since the logarithm of zero is undefined.
```{r}
vars_con<-names(df)[c(5,7,8,11)]
vars_res<-names(df)[c(3,18)]
vars_dis<-names(df)[c(1,2,4,6,9,10,13,14,15,16,17)]
ll<-which(df$years_sell==0);ll
df$years_sell[ll]<-0.5

ll<-which(df$tax==0);ll
df$tax[ll]<-0.5

ll<-which(df$mileage==0);ll
df$mileage[ll]<-0.5

ll<-which(df$mpg==0);ll
df$mpg[ll]<-0.5
```

# Linear Models: Using numerical explanatory variables
We do linear regression in order to predict the value of price variable based on/according to the values of other variables.
```{r}
m1<-lm(price~mileage+tax+mpg+years_sell,data=df)
summary(m1)
vif(m1)
par(mfrow=c(2,2))
plot(m1,id.n=0)
residualPlots(m1,id=list(method=cooks.distance(m1),n=10)) 
```

As we can see all the chosen variables have coefficients different from zero, i.e. they are useful, because their p-values are less than 0.05.
These explanatory variables explain a 52.4 % of the target's variability. 
Also, we can see that there are no collinear variables, i.e., highly correlated variables.

However, we can see that residuals are neither homoscedastic nor normal. So, we have to apply transformations. Also, the clearest nonlinear variable is mpg. 

## Target transformation
```{r}
library(MASS)
# Target variable transformation?
par(mfrow=c(1,1))
boxcox(price~mileage+tax+mpg+years_sell,data=df)
# Lambda=0 - log transformation is needed

# New model:
m2<-lm(log(price)~mileage+tax+mpg+years_sell,data=df)
summary(m2)
vif(m2) 
par(mfrow=c(2,2))
plot(m2, id.n=0)
residualPlots(m2,id=list(method=cooks.distance(m2),n=10)) 
```

As we can see the transformation needed is a logarithmic transformation, because λ ≈ 0. 

Now, we can see that tax variable is not useful, because its p-value is greater than 0.05. However, the explantory variables explain a 61.18 % of the target's variability.
There are still no collinear variables because they have not changed. 

However, the residuals continue to be neither homoscedastic nor normal. So, more/further transformations are needed/required. Also, the clearest nonlinear variables are mpg and years_sell. 

This model is better than the previous one.
It seems that this model is better than the previous one.

## Explanatory variables transformation
```{r}
boxTidwell(log(price)~mileage+tax+mpg, data=df[!df$mout=="YesMOut",])
#?boxTidwell
```

We need to cube the tax variable, take the square root of the mileage variable and cube, take the square root and raise to minus one the mpg variable.

```{r}
#df2 <- df[!df$mout=="YesMOut",]

m3<-lm(log(price)~sqrt(mileage)+poly(tax,3)+ I(mpg^(-1/2))+years_sell,data=df[!df$mout=="YesMOut",])#DECIDIR, I(mpg^(-3/2))
m3<-lm(log(price)~sqrt(mileage)+poly(tax,3)+ I(mpg^(-1/3))+years_sell,data=df[!df$mout=="YesMOut",])
m3<-lm(log(price)~sqrt(mileage)+poly(tax,3)+ I(mpg^(-3/2))+years_sell,data=df[!df$mout=="YesMOut",])

summary(m3)
vif(m3) 
par(mfrow=c(2,2))
plot(m3, id.n=0)
residualPlots(m3,id=list(method=cooks.distance(m3),n=10)) 
```

As we can see all the chosen variables have coefficients different from zero, i.e. they are useful, because their p-values are less than 0.05.
These explanatory variables explain a 56.48, 56.47%, 54.56 % of the target's variability. 
We can say that the price and tax variables are collinear, i.e., they are highly correlated. However, this is because we have adapted them.

Also, we can see that residuals are now homoscedastic and normal. However, we can see some influential observation. Also, there are not nonlinear variables. 

This model is better than the previous one.
It seems that this model is better than the previous one.

## Factors

```{r}
res.condes<-condes(df,3)
res.condes$quali
```

As we can see the two most significant factors are model, year and engineSize, because they have the highest R^2. 

```{r}
# Put some reasonable limits to initial model complexity.
#df$engineSize_num <- 0
#df$engineSize <- as.numeric(as.character(df$engineSize))
#df$engineSize<-as.numeric(df$engineSize)
#df[which(df$engineSize <= 2),"engineSize_num"] <- "small_engine"
#df[which((df$engineSize) > 2 & (df$engineSize <= 3)),"engineSize_num"] <- "medium_engine"
#df[which(df$engineSize > 3),"engineSize_num"] <- "large_engine"
#df$engineSize_num <- factor( df$engineSize_num, levels = c("small_engine","medium_engine","large_engine")) 
#df$engineSize_num<-factor(df$engineSize_num)

df$engineSize_num<-as.numeric(df$engineSize)
df$engineSize<-as.numeric(df$engineSize)
df[which(df$engineSize_num<=2),"engineSize_num"] <- "small_engine"
df[which((df$engineSize_num>2) & (df$engineSize_num<=3)),"engineSize_num"] <- "medium_engine"
df[which(df$engineSize_num>3),"engineSize_num"] <- "large_engine"
df$engineSize_num<-factor(df$engineSize_num)
levels(df$engineSize)

m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[!df$mout=="YesMOut",])#model problemes, obs. leverage elevat
summary(m4)
par(mfrow=c(1,1))
#acf(rstudent(m4))
par(mfrow=c(2,2))
plot
residualPlots(m4,id=list(method=cooks.distance(m4),n=10)) 

```

These explanatory variables explain an  % of the target's variability. 

Also, we can see that residuals are now homoscedastic and normal. However, we can see some influential observation. 



## Diagnostics for numeric variables

```{r}
# Define initial parameters:
p <-length(m4$coefficients)
n <- length(m4$fitted.values)
h_param <- 3

# Residual analysis:
llres <- which(abs(rstudent(m4))>3);llres
influencePlot(m4, id=list(n=10))
Boxplot(cooks.distance(m4),id=list(labels=row.names(df)))

# A priori influential observation
ll_priori_influential <- which(abs(hatvalues(m4))>h_param*(p/n))
length(ll_priori_influential)

# A posteriori influential observation:
ll_posteriori_influential<-which(abs(cooks.distance(m4))>(4/(n-p)));length(ll_posteriori_influential)
ll_unique_influential<-unique(c(ll_priori_influential,ll_posteriori_influential));length(ll_unique_influential)

m4 <- update(m3, ~.+transmission+fuelType+manufacturer+engineSize_num,data=df[(!df$mout=="YesMOut"&-ll_posteriori_influential),])
summary(m4)
```


